\section{Introduction}
New approaches to understanding music cognition come at a time when digital music services such as Apple, Google, Amazon, and Spotify are seeing unprecedented growth within the music industry. These approaches are best embodied by the computationally focused field of Music Information Retrieval (MIR) \cite{downie2003music}. A fundamental task of MIR is the development of acoustic feature extractors that automatically capture unique characteristics from a recorded piece of sound. However, some acoustic features may not be wholly represented in the acoustic signal, and MIR has been criticized for failing to model their analysis based on psychological research \cite{aucouturier2013seven}. For example, ``danceability'' is a feature available in signal processing packages \cite{bogdanov2013essentia,bogdanov2009low} and open-access APIs\footnote{\url{https://developer.spotify.com/web-api/get-audio-features/}} using a combination of features such as beat salience and consistency to derive a metric between zero and one \cite{streich2005detrended}. However, according to this definition the most danceable song should be a steady, metronomic-like pulse, which clearly does not capture the perceptual nuances of what makes music danceable \cite{friberg2014using}. It may be more accurate to say that the danceability feature is closer to the ability to detect the beat of an audio sample. 

The inclusion of psychological acoustic features using signal-only analysis is surprising, given that music is dynamic system influenced by cognitive \cite{koelsch2005towards}, cultural, market and political forces \cite{bielby2004managing}. Despite this knowledge, research is relatively sparse as to how, or to what degree, specific features influence musical preference. This work examines the extent to which an interdisciplinary approach using cognitive psychology, signal processing, and economic decision making can be used to investigate unexplored psychological perception of ``singability'': the degree to which a recorded piece of music is attractive for singing. Although this problem may appear trivial at face value, it is an actively evolving area of research at the intersection of music cognition and computation. On average, US citizens spend an average of 4 hours a day listening to music \cite{webster2015share}, and 18-34 year olds spent \$163 per year on music events and services.\footnote{\url{http://www.digitalmusicnews.com/2016/01/08/one-third-of-all-music-spending-goes-to-live-events/}} Digital karaoke applications such as Smule's Sing \footnote{\url{https://www.smule.com/listen/sing-karaoke/8}} have become increasingly popular in recent years, reaching over 50 million of users every month worldwide.\footnote{\url{https://www.forbes.com/sites/mnewlands/2016/09/20/smule-has-changed-the-music-industry-completely-heres-how/\#1f8e1fb041c0}}. Companies like Smule and Spotify need established representations of music cognition in order maximize user satisfaction through automated machine learning algorithms. However, despite years of traditional scientific research from cognitive psychology \cite{pinker1999mind,huron2001music,cross2012music} anthropology \cite{mithen2006singing,patel2003language}, neuroscience \cite{peretz2005brain}, and musicology \cite{wallin2001origins}, conclusive understanding of fundamental aspects of music cognition and preference remains obscure. Endeavours which meet at the intersection of traditional music cognition and computational MIR may address the longstanding lacuna in academic progress in the understanding of music perception. Because most MIR extractors consider only the acoustic signal and ignore important behavioural information that could improve classification, the objective of this research is to address this lacuna by exploring whether a psychological feature of music like singability can be derived from an acoustic signal with behavioural labels using an integrated approach. To date, no research has been conducted which explores whether a feature such as singability can be extracted from music.

Determining a complex psychological process and decision making strategy like singability is a difficult task. To start, it is initially unclear how to quantify such a subjective multiple criterion choice in a controlled, scientific manner; how does one determine how familiarity and producibility impact the final choice to sing or how they interact with each other? Furthermore, relatively little research has attempted to quantify singing preference, and the preference literature in other species such as from birds does not transfer well to humans. Because singability will likely not contain a universally agreed upon set of factors, the major challenge is how to quantify which, or to what degree, these factors should be incorporated into a model for evaluation. We examine the concept of singability using a synthesis of multiple-criterion decision making processes, acoustic feature extraction, and machine learning founded on a theoretical background of music cognition.

